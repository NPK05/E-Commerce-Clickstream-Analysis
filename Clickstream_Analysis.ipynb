{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb8e383",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ E-Commerce Clickstream Analysis\n",
    "A comprehensive project for preprocessing clickstream data, modeling churn, detecting anomalies, and building recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fb14c",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4aea12",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Step 1: Preprocessing Clickstream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load dataset\n",
    "data = pd.read_csv(\"ecommerce_clickstream_transactions_enhanced.csv\")\n",
    "\n",
    "# Step 2: Combine Date and Timestamp into FullTimestamp\n",
    "def clean_full_timestamp(row):\n",
    "    try:\n",
    "        date_str = f\"{int(row['Year'])}-{int(row['Month']):02d}-{int(row['Day']):02d}\"\n",
    "        time_str = \"00:\" + str(row['Timestamp'])  # Make HH:MM:SS\n",
    "        return pd.to_datetime(f\"{date_str} {time_str}\", errors='coerce')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "data['FullTimestamp'] = data.apply(clean_full_timestamp, axis=1)\n",
    "data = data.dropna(subset=['FullTimestamp'])\n",
    "\n",
    "# Step 3: Sort and compute time differences\n",
    "data = data.sort_values(by=['UserID', 'FullTimestamp']).reset_index(drop=True)\n",
    "data['TimeDiff_Seconds'] = data.groupby('UserID')['FullTimestamp'].diff().dt.total_seconds().fillna(0)\n",
    "data['TimeDiff_Hours'] = data['TimeDiff_Seconds'] / 3600\n",
    "\n",
    "# Step 4: Identify sessions\n",
    "data['NewSession'] = (data['TimeDiff_Seconds'] > 1800).astype(int)\n",
    "data['SessionID'] = data.groupby('UserID')['NewSession'].cumsum()\n",
    "\n",
    "# Step 5: Calculate Session Duration\n",
    "session_info = data.groupby(['UserID', 'SessionID'])['FullTimestamp'].agg(['min', 'max']).reset_index()\n",
    "session_info['SessionDuration'] = (session_info['max'] - session_info['min']).dt.total_seconds()\n",
    "data = data.merge(session_info[['UserID', 'SessionID', 'SessionDuration']], on=['UserID', 'SessionID'], how='left')\n",
    "\n",
    "# Step 6: Ensure FullTimestamp is datetime\n",
    "data['FullTimestamp'] = pd.to_datetime(data['FullTimestamp'], errors='coerce', utc=True)\n",
    "\n",
    "\n",
    "# Step 7: Extract time features\n",
    "data['Weekday'] = data['FullTimestamp'].dt.dayofweek\n",
    "data['HourOfDay'] = data['FullTimestamp'].dt.hour\n",
    "\n",
    "# Step 8: Encode target variable\n",
    "data['Made_Purchase'] = np.where(data['EventType'] == 'purchase', 1, 0)\n",
    "\n",
    "\n",
    "# Step 10: Scale numerical features\n",
    "features_to_scale = [\n",
    "    'SessionDuration', 'TimeDiff_Seconds', 'HourOfDay', 'Weekday',\n",
    "    'ProductCount', 'EventCount', 'DaysSinceLastPurchase',\n",
    "    'recency', 'frequency', 'monetary'\n",
    "]\n",
    "for col in features_to_scale:\n",
    "    if col not in data.columns:\n",
    "        data[col] = 0\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[features_to_scale] = scaler.fit_transform(data[features_to_scale])\n",
    "\n",
    "# Step 11: Drop intermediate helper column\n",
    "data.drop(columns=['NewSession'], inplace=True)\n",
    "\n",
    "# Step 12: Save final preprocessed data\n",
    "data.to_csv(\"final_preprocessed_data.csv\", index=False)\n",
    "print(\"âœ… Preprocessing complete. Final dataset saved as 'final_preprocessed_data.csv'\")\n",
    "print(\"Final shape:\", data.shape)\n",
    "print(\"ðŸ“Š Final Preprocessing Check\")\n",
    "\n",
    "# 1. Show dataset shape\n",
    "print(f\"\\nâœ… Dataset shape: {data.shape}\")\n",
    "\n",
    "# 2. List columns\n",
    "print(f\"\\nðŸ“‹ Columns: {list(data.columns)}\")\n",
    "\n",
    "# 3. Check for missing values\n",
    "print(\"\\nðŸ” Missing values:\")\n",
    "print(data.isnull().sum()[data.isnull().sum() > 0])\n",
    "\n",
    "# 4. Check datatypes\n",
    "print(\"\\nðŸ§  Data types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 5. Quick sample of processed data\n",
    "print(\"\\nðŸ”Ž Sample rows:\")\n",
    "print(data.head(3))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… Step 1: Make sure FullTimestamp is datetime\n",
    "data['FullTimestamp'] = pd.to_datetime(data['FullTimestamp'], errors='coerce')\n",
    "\n",
    "# âœ… Step 2: Define a reference date (latest timestamp in the dataset)\n",
    "reference_date = data['FullTimestamp'].max()\n",
    "\n",
    "# âœ… Step 3: Calculate RFM for each UserID\n",
    "rfm = data.groupby('UserID').agg(\n",
    "    recency = ('FullTimestamp', lambda x: (reference_date - x.max()).days),\n",
    "    frequency = ('SessionID', 'nunique'),\n",
    "    monetary = ('Amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# âœ… Step 4: Merge RFM back to the main dataset\n",
    "data = data.merge(rfm, on='UserID', how='left')\n",
    "\n",
    "# âœ… Optional: Display results\n",
    "print(\"âœ… RFM Features Added\")\n",
    "print(data[['UserID', 'recency', 'frequency', 'monetary']].head())\n",
    "\n",
    "data.to_csv(\"final_with_rfm.csv\", index=False)\n",
    "print(\"âœ… File saved as 'final_with_rfm.csv'\")\n",
    "# Customer Segmentation Using PCA and K-Means Clustering with Business Insights\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load Data\n",
    "rfm_data = pd.read_csv(\"final_with_rfm.csv\")\n",
    "\n",
    "# Keep only RFM relevant attributes\n",
    "rfm_df = rfm_data[['UserID', 'recency', 'frequency', 'monetary']]\n",
    "\n",
    "# Step 1: Standardize Data\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_df[['recency', 'frequency', 'monetary']])\n",
    "\n",
    "# Step 2: PCA Analysis & Visualization\n",
    "pca = PCA(n_components=3)\n",
    "rfm_pca = pca.fit_transform(rfm_scaled)\n",
    "\n",
    "# Explained variance plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Finding Optimal k using Elbow and Silhouette Methods\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(rfm_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(rfm_pca, labels))\n",
    "\n",
    "# Visualization for optimal k\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Elbow method plot (Inertia)\n",
    "ax1.plot(k_range, inertia, marker='o', linestyle='-', color='blue')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Silhouette Score Plot\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(k_range, silhouette_scores, marker='s', linestyle='--', color='red')\n",
    "ax2.set_ylabel('Silhouette Score', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title('Elbow Method and Silhouette Score Analysis')\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print Silhouette scores explicitly\n",
    "print(\"Silhouette Scores by Cluster Number:\")\n",
    "for k, score in zip(k_range, silhouette_scores):\n",
    "    print(f\"{k} clusters: {score:.4f}\")\n",
    "\n",
    "# Choose optimal k (highest silhouette score)\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "print(f\"\\nOptimal number of clusters chosen: {optimal_k}\")\n",
    "\n",
    "# Step 4: Perform KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm_df['Cluster'] = kmeans.fit_predict(rfm_pca)\n",
    "\n",
    "# Step 5: Business Insights - Cluster Profiling\n",
    "cluster_summary = rfm_df.groupby('Cluster').mean().round(2)\n",
    "print(\"\\nCluster Profiling (Business Insights):\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Visualization: PCA Cluster Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=rfm_pca[:, 0], y=rfm_pca[:, 1], hue=rfm_df['Cluster'], palette='Set2', s=50, alpha=0.7)\n",
    "plt.title('Customer Segments Visualization (PCA Components)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Assign Marketing Actions Based on Cluster Characteristics\n",
    "marketing_campaigns = {\n",
    "    0: \"VIP Customers: Exclusive discounts & early sales access.\",\n",
    "    1: \"Loyal Customers: Personalized product recommendations.\",\n",
    "    2: \"At-Risk Customers: Win-back campaigns with incentives.\",\n",
    "    3: \"Big Spenders: Premium upselling opportunities.\",\n",
    "    4: \"New/Inactive Users: Engagement & onboarding initiatives.\"\n",
    "}\n",
    "\n",
    "rfm_df['Marketing_Action'] = rfm_df['Cluster'].map(marketing_campaigns)\n",
    "\n",
    "# Step 7: Save clusters into separate CSV files (only relevant columns)\n",
    "relevant_columns = ['UserID', 'recency', 'frequency', 'monetary', 'Cluster', 'Marketing_Action']\n",
    "\n",
    "for cluster in rfm_df['Cluster'].unique():\n",
    "    filename = f\"cluster_{cluster}_customers.csv\"\n",
    "    cluster_data = rfm_df[rfm_df['Cluster'] == cluster][relevant_columns]\n",
    "    cluster_data.to_csv(filename, index=False)\n",
    "\n",
    "# Final consolidated file with marketing actions\n",
    "rfm_df[relevant_columns].to_csv(\"customer_marketing_actions.csv\", index=False)\n",
    "\n",
    "# Final cluster summary visualization (bar chart)\n",
    "cluster_summary.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average RFM values per Cluster')\n",
    "plt.ylabel('Average Value')\n",
    "plt.xlabel('Cluster')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(['Recency', 'Frequency', 'Monetary'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Load your final dataset with clusters & marketing actions\n",
    "rfm_df = pd.read_csv(\"customer_marketing_actions.csv\")\n",
    "\n",
    "# Count customers by marketing action\n",
    "action_counts = rfm_df['Marketing_Action'].value_counts().reset_index()\n",
    "action_counts.columns = ['Marketing_Action', 'Customer_Count']\n",
    "\n",
    "# Bar plot to visualize clearly\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Customer_Count', y='Marketing_Action', data=action_counts, palette='viridis')\n",
    "plt.title(\"Customer Count by Marketing Action\", fontsize=16)\n",
    "plt.xlabel(\"Number of Customers\")\n",
    "plt.ylabel(\"Marketing Action\")\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Example Data\n",
    "data = {\n",
    "    'Marketing Action': [\n",
    "        'Loyal Customers',\n",
    "        'VIP Customers',\n",
    "        'At-Risk Customers'\n",
    "    ],\n",
    "    'Customer Count': [29500, 22500, 7000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define Poster Color Palette (Navy, Orange, Yellow)\n",
    "poster_colors = ['#1C1E5A', '#FF6B35', '#FFC857']  # navy, bright orange, warm yellow\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = sns.barplot(\n",
    "    y='Marketing Action',\n",
    "    x='Customer Count',\n",
    "    data=df,\n",
    "    palette=poster_colors\n",
    ")\n",
    "\n",
    "plt.title(\"Customer Count by Marketing Action\", fontsize=14)\n",
    "plt.xlabel(\"Number of Customers\")\n",
    "plt.ylabel(\"Marketing Action\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize recency distribution clearly\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['recency'], bins=30, kde=True)\n",
    "plt.title('Distribution of Recency (days since last interaction)')\n",
    "plt.xlabel('Recency (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Check basic statistics for recency\n",
    "print(data['recency'].describe(percentiles=[0.50, 0.75, 0.90, 0.95, 0.99]))\n",
    "rfm_data['Churn'] = rfm_data['recency'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Confirm churn distribution clearly\n",
    "print(rfm_data['Churn'].value_counts())\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "rfm_data['Churn'] =rfm_data['recency'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Only relevant columns\n",
    "check_columns = ['frequency', 'monetary', 'SessionDuration', 'EventCount', 'ProductCount', 'Churn']\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(rfm_data[check_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap clearly showing relationships\")\n",
    "plt.show()\n",
    "# Clearly checking feature distribution by churn class\n",
    "features = ['frequency', 'monetary', 'SessionDuration', 'EventCount', 'ProductCount']\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.boxplot(x='Churn', y=feature, data=rfm_data)\n",
    "    plt.title(f\"{feature} by Churn Status (0=Active, 1=Churn)\")\n",
    "    plt.show()\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "features = ['frequency', 'monetary', 'SessionDuration', 'EventCount', 'ProductCount']\n",
    "X = rfm_data[features]\n",
    "y = rfm_data['Churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (Neural Networks require it!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Step 4: Train Models (focus on best performers)\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'Naive Bayes': GaussianNB() }\n",
    " \n",
    "\n",
    "# ðŸ“Š Step 5: Evaluation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "# ðŸ† Step 6: Choose Best Model (Random Forest)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# âœ… Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Active', 'Churn'], yticklabels=['Active', 'Churn'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Churn Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ§  Feature Importance (Random Forest)\n",
    "importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8, 5))\n",
    "importances.plot(kind='barh')\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ’¾ Save the best model\n",
    "joblib.dump(rf, \"random_forest_churn_model.pkl\")\n",
    "print(\"âœ… Model saved as 'random_forest_churn_model.pkl'\")\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,         # 3-fold cross-validation\n",
    "    n_jobs=-1,    # use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model & parameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"\\nâœ… Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "y_prob_best = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nðŸ“Š Classification Report (Tuned Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob_best))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Active', 'Churn'], yticklabels=['Active', 'Churn'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Churn defined without leakage\n",
    "rfm_data['Churn'] =rfm_data['recency'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Clearly remove data-leaking features (recency)\n",
    "features = ['frequency', 'monetary', 'SessionDuration', 'EventCount', 'ProductCount']\n",
    "\n",
    "X = rfm_data[features]\n",
    "y = rfm_data['Churn']\n",
    "\n",
    "# Split dataset clearly with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scaling features\n",
    "pipeline_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Train clearly\n",
    "pipeline_svm.fit(X_train, y_train)\n",
    "y_pred_svm = pipeline_svm.predict(X_test)\n",
    "y_pred_prob_svm = pipeline_svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate clearly\n",
    "print(\"Improved SVM Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_prob_svm))\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# âœ… Step 5: Handle class imbalance using class weights\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(weights))\n",
    "\n",
    "# âœ… Step 6: Build the model\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# âœ… Step 7: Train model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# âœ… Step 8: Predict and evaluate\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred_labels = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nðŸ§  Classification Report (Neural Network):\")\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_probs))\n",
    "\n",
    "# âœ… Step 9: Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Active', 'Churn'], yticklabels=['Active', 'Churn'])\n",
    "plt.title(\"Neural Network - Confusion Matrix (Balanced)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… Step 10: ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_probs):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Neural Network - ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# âœ… Step 11: Training loss curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Step 1: Load clickstream data\n",
    "clickstream_df = pd.read_csv(\"ecommerce_clickstream_transactions_enhanced.csv\")\n",
    "\n",
    "# Step 2: Define funnel stages\n",
    "funnel_stages = ['login', 'page_view', 'product_view', 'click', 'add_to_cart', 'purchase']\n",
    "\n",
    "# Step 3: Filter and sort events\n",
    "funnel_df = clickstream_df[clickstream_df['EventType'].isin(funnel_stages)].copy()\n",
    "funnel_df['Timestamp'] = pd.to_datetime(funnel_df['Timestamp'])\n",
    "funnel_df = funnel_df.sort_values(by=['UserID', 'Timestamp'])\n",
    "\n",
    "# Step 4: Get first timestamp for each funnel stage per user\n",
    "first_stage_times = funnel_df.groupby(['UserID', 'EventType'])['Timestamp'].min().unstack()\n",
    "\n",
    "# Step 5: Apply funnel progression logic\n",
    "def check_funnel_progression(row):\n",
    "    progression = {}\n",
    "    for i, stage in enumerate(funnel_stages):\n",
    "        if i == 0:\n",
    "            progression[stage] = pd.notna(row[stage])\n",
    "        else:\n",
    "            prev_stage = funnel_stages[i - 1]\n",
    "            progression[stage] = (\n",
    "                pd.notna(row[stage]) and\n",
    "                pd.notna(row[prev_stage]) and\n",
    "                row[stage] >= row[prev_stage]\n",
    "            )\n",
    "    return pd.Series(progression)\n",
    "\n",
    "funnel_flags = first_stage_times.apply(check_funnel_progression, axis=1)\n",
    "funnel_flags['UserID'] = first_stage_times.index\n",
    "user_funnel = funnel_flags[['UserID'] + funnel_stages].reset_index(drop=True)\n",
    "\n",
    "# Step 6: Create summary table\n",
    "funnel_summary_df = pd.DataFrame({\n",
    "    'Stage': funnel_stages,\n",
    "    'Users Reached': [user_funnel[stage].sum() for stage in funnel_stages]\n",
    "})\n",
    "total_users = user_funnel.shape[0]\n",
    "print(\"âœ… Funnel Summary:\")\n",
    "print(funnel_summary_df)\n",
    "print(f\"\\nTotal Users: {total_users}\")\n",
    "\n",
    "# Step 7: Drop-off and conversion analysis\n",
    "drop_off_data = []\n",
    "for i in range(len(funnel_stages) - 1):\n",
    "    current_stage = funnel_stages[i]\n",
    "    next_stage = funnel_stages[i + 1]\n",
    "\n",
    "    entered = user_funnel[user_funnel[current_stage]].shape[0]\n",
    "    reached_next = user_funnel[user_funnel[current_stage] & user_funnel[next_stage]].shape[0]\n",
    "    dropped = entered - reached_next\n",
    "    conversion_rate = (reached_next / entered) * 100 if entered else 0\n",
    "\n",
    "    drop_off_data.append({\n",
    "        'From â†’ To': f\"{current_stage} â†’ {next_stage}\",\n",
    "        'Users Entered': entered,\n",
    "        'Reached Next Stage': reached_next,\n",
    "        'Dropped Off': dropped,\n",
    "        '% Conversion': round(conversion_rate, 2)\n",
    "    })\n",
    "\n",
    "drop_off_df = pd.DataFrame(drop_off_data)\n",
    "print(\"\\nâœ… Drop-Off Analysis:\")\n",
    "print(drop_off_df)\n",
    "\n",
    "# Step 8: Visualize funnel\n",
    "fig = go.Figure(go.Funnel(\n",
    "    y=funnel_summary_df['Stage'],\n",
    "    x=funnel_summary_df['Users Reached'],\n",
    "    textinfo=\"value+percent previous+percent initial\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ðŸ“Š Realistic User Funnel (Progression by Stage)\",\n",
    "    font=dict(size=14),\n",
    "    margin=dict(l=80, r=80, t=50, b=50)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Funnel stages and values\n",
    "stages = ['login', 'page_view', 'product_view', 'click', 'add_to_cart', 'purchase']\n",
    "counts = [1000, 521, 494, 512, 490, 470]\n",
    "\n",
    "# Funnel chart with custom colors matching poster theme\n",
    "fig = go.Figure(go.Funnel(\n",
    "    y=stages,\n",
    "    x=counts,\n",
    "    textinfo=\"value+percent initial+percent previous\",\n",
    "    marker={\"color\": [\"#FF6F61\", \"#FFA351\", \"#F6D860\", \"#66BFBF\", \"#6A67CE\", \"#3D348B\"]}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Realistic User Funnel (Progression by Stage)\",\n",
    "    font=dict(size=16),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "cluster_df = pd.read_csv(\"customer_marketing_actions.csv\")[['UserID', 'Cluster']]\n",
    "\n",
    "stage_times = funnel_df.groupby(['UserID', 'EventType'])['Timestamp'].min().unstack().reset_index()\n",
    "# === Step 8: Time-to-Conversion (per user) ===\n",
    "time_to_conversion = pd.DataFrame({'UserID': stage_times['UserID']})\n",
    "for i in range(1, len(funnel_stages)):\n",
    "    start, end = funnel_stages[i - 1], funnel_stages[i]\n",
    "    delta = (stage_times[end] - stage_times[start]).dt.total_seconds() / 60\n",
    "    time_to_conversion[f\"{start} â†’ {end} (min)\"] = delta\n",
    "\n",
    "# === Step 9: Average Time Between Stages (Global) ===\n",
    "cleaned_time_df = time_to_conversion.copy()\n",
    "for col in cleaned_time_df.columns[1:]:\n",
    "    cleaned_time_df = cleaned_time_df[cleaned_time_df[col] >= 0]\n",
    "avg_time_per_stage = cleaned_time_df.drop(columns='UserID').mean().div(60).round(2)\n",
    "\n",
    "# === Step 10: Cluster-Segmented Avg Times ===\n",
    "stage_times_clustered = pd.merge(stage_times, cluster_df, on='UserID', how='inner')\n",
    "cluster_avg_times = {}\n",
    "for cluster in sorted(stage_times_clustered['Cluster'].unique()):\n",
    "    group = stage_times_clustered[stage_times_clustered['Cluster'] == cluster]\n",
    "    transitions = {}\n",
    "    for i in range(1, len(funnel_stages)):\n",
    "        s, e = funnel_stages[i - 1], funnel_stages[i]\n",
    "        delta = (group[e] - group[s]).dt.total_seconds() / 60\n",
    "        delta = delta[delta >= 0]\n",
    "        transitions[f\"{s} â†’ {e}\"] = round(delta.mean() / 60, 2)\n",
    "    cluster_avg_times[f\"Cluster {cluster}\"] = transitions\n",
    "cluster_avg_time_df = pd.DataFrame(cluster_avg_times).T\n",
    "\n",
    "# === Step 11: Visualize Funnel ===\n",
    "fig = go.Figure(go.Funnel(\n",
    "    y=funnel_summary_df['Stage'],\n",
    "    x=funnel_summary_df['Users Reached'],\n",
    "    textinfo=\"value+percent previous+percent initial\"\n",
    "))\n",
    "fig.update_layout(title=\"Realistic Funnel Progression\", font=dict(size=14))\n",
    "fig.show()\n",
    "\n",
    "# === Step 12: Visualize Avg Time by Cluster ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_avg_time_df.T.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Avg Time Between Funnel Stages by Cluster (in Hours)\")\n",
    "plt.ylabel(\"Hours\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "purchase_data = clickstream_df[clickstream_df['EventType'] == 'purchase']\n",
    "\n",
    "# === Step 1: Build User-Product Matrix ===\n",
    "user_product_matrix = purchase_data.groupby(['UserID', 'ProductID']).size().unstack(fill_value=0)\n",
    "\n",
    "# === Step 2: Collaborative Filtering (TruncatedSVD) ===\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(user_product_matrix)\n",
    "user_similarity = cosine_similarity(user_factors)\n",
    "\n",
    "# === Step 3: Co-Occurrence Matrix ===\n",
    "user_purchases = purchase_data.groupby('UserID')['ProductID'].apply(set)\n",
    "co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for products in user_purchases:\n",
    "    for prod_a, prod_b in combinations(products, 2):\n",
    "        co_occurrence[prod_a][prod_b] += 1\n",
    "        co_occurrence[prod_b][prod_a] += 1\n",
    "\n",
    "# === Step 4: Target User Selection ===\n",
    "user_ids = user_product_matrix.index\n",
    "sample_user_index = 0\n",
    "sample_user_id = user_ids[sample_user_index]\n",
    "user_products = user_purchases[sample_user_id]\n",
    "\n",
    "# --- A: Collaborative Filtering Recommendations ---\n",
    "similar_user_indices = user_similarity[sample_user_index].argsort()[::-1][1:6]\n",
    "similar_user_ids = user_ids[similar_user_indices]\n",
    "\n",
    "collab_scores = user_product_matrix.loc[similar_user_ids].sum()\n",
    "collab_scores = collab_scores.drop(labels=user_products, errors='ignore')\n",
    "collab_scores = collab_scores[collab_scores > 0].sort_values(ascending=False)\n",
    "collab_df = collab_scores.reset_index()\n",
    "collab_df.columns = ['ProductID', 'Collaborative Score']\n",
    "\n",
    "# --- B: Co-Occurrence Recommendations ---\n",
    "cooccur_scores = defaultdict(int)\n",
    "for product in user_products:\n",
    "    for related, score in co_occurrence[product].items():\n",
    "        if related not in user_products:\n",
    "            cooccur_scores[related] += score\n",
    "\n",
    "cooccur_df = pd.DataFrame.from_dict(cooccur_scores, orient='index', columns=['Co-Occurrence Score'])\n",
    "cooccur_df = cooccur_df.sort_values(by='Co-Occurrence Score', ascending=False).reset_index()\n",
    "cooccur_df.columns = ['ProductID', 'Co-Occurrence Score']\n",
    "\n",
    "# --- C: Hybrid Ranking ---\n",
    "hybrid_df = pd.merge(collab_df, cooccur_df, on='ProductID', how='outer').fillna(0)\n",
    "hybrid_df['Hybrid Score'] = hybrid_df['Collaborative Score'] + hybrid_df['Co-Occurrence Score']\n",
    "hybrid_df = hybrid_df.sort_values(by='Hybrid Score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# === Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(hybrid_df['ProductID'], hybrid_df['Hybrid Score'], color='purple')\n",
    "plt.xlabel(\"Hybrid Score\")\n",
    "plt.title(f\"Top Recommended Products for User {sample_user_id}\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Display Result ===\n",
    "print(f\"âœ… Top Hybrid Recommendations for User {sample_user_id}:\\n\")\n",
    "print(hybrid_df)\n",
    "# === Step 1: Load preprocessed clickstream data ===\n",
    "df = pd.read_csv(\"final_preprocessed_data.csv\")\n",
    "\n",
    "# === Step 2: Focus on 'product_view' events only ===\n",
    "product_views = df[df['EventType'] == 'product_view']\n",
    "\n",
    "# === Step 3: Count how many times each user viewed each product ===\n",
    "view_counts = product_views.groupby(['UserID', 'ProductID']).size().reset_index(name='ViewCount')\n",
    "\n",
    "# === Step 4: Sort by highest view count (intent) ===\n",
    "sorted_views = view_counts.sort_values(by='ViewCount', ascending=False)\n",
    "\n",
    "# === Step 5: Get top product (most-viewed) for 10 unique users ===\n",
    "top_10_intent_unique = sorted_views.drop_duplicates(subset='UserID', keep='first').head(10)\n",
    "\n",
    "# === Step 6: Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_intent_unique['UserID'].astype(str), top_10_intent_unique['ViewCount'], color='teal')\n",
    "plt.xlabel(\"View Count\")\n",
    "plt.ylabel(\"UserID\")\n",
    "plt.title(\"ðŸŽ¯ Top 10 High-Intent Users and Their Most Viewed Products\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Step 7: Print final recommendation table ===\n",
    "print(\"âœ… Recommended Products for High-Intent Users:\")\n",
    "print(top_10_intent_unique)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cf200",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 2: Funnel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Step 12: Visualize Avg Time by Cluster ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_avg_time_df.T.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Avg Time Between Funnel Stages by Cluster (in Hours)\")\n",
    "plt.ylabel(\"Hours\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "purchase_data = clickstream_df[clickstream_df['EventType'] == 'purchase']\n",
    "\n",
    "# === Step 1: Build User-Product Matrix ===\n",
    "user_product_matrix = purchase_data.groupby(['UserID', 'ProductID']).size().unstack(fill_value=0)\n",
    "\n",
    "# === Step 2: Collaborative Filtering (TruncatedSVD) ===\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(user_product_matrix)\n",
    "user_similarity = cosine_similarity(user_factors)\n",
    "\n",
    "# === Step 3: Co-Occurrence Matrix ===\n",
    "user_purchases = purchase_data.groupby('UserID')['ProductID'].apply(set)\n",
    "co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for products in user_purchases:\n",
    "    for prod_a, prod_b in combinations(products, 2):\n",
    "        co_occurrence[prod_a][prod_b] += 1\n",
    "        co_occurrence[prod_b][prod_a] += 1\n",
    "\n",
    "# === Step 4: Target User Selection ===\n",
    "user_ids = user_product_matrix.index\n",
    "sample_user_index = 0\n",
    "sample_user_id = user_ids[sample_user_index]\n",
    "user_products = user_purchases[sample_user_id]\n",
    "\n",
    "# --- A: Collaborative Filtering Recommendations ---\n",
    "similar_user_indices = user_similarity[sample_user_index].argsort()[::-1][1:6]\n",
    "similar_user_ids = user_ids[similar_user_indices]\n",
    "\n",
    "collab_scores = user_product_matrix.loc[similar_user_ids].sum()\n",
    "collab_scores = collab_scores.drop(labels=user_products, errors='ignore')\n",
    "collab_scores = collab_scores[collab_scores > 0].sort_values(ascending=False)\n",
    "collab_df = collab_scores.reset_index()\n",
    "collab_df.columns = ['ProductID', 'Collaborative Score']\n",
    "\n",
    "# --- B: Co-Occurrence Recommendations ---\n",
    "cooccur_scores = defaultdict(int)\n",
    "for product in user_products:\n",
    "    for related, score in co_occurrence[product].items():\n",
    "        if related not in user_products:\n",
    "            cooccur_scores[related] += score\n",
    "\n",
    "cooccur_df = pd.DataFrame.from_dict(cooccur_scores, orient='index', columns=['Co-Occurrence Score'])\n",
    "cooccur_df = cooccur_df.sort_values(by='Co-Occurrence Score', ascending=False).reset_index()\n",
    "cooccur_df.columns = ['ProductID', 'Co-Occurrence Score']\n",
    "\n",
    "# --- C: Hybrid Ranking ---\n",
    "hybrid_df = pd.merge(collab_df, cooccur_df, on='ProductID', how='outer').fillna(0)\n",
    "hybrid_df['Hybrid Score'] = hybrid_df['Collaborative Score'] + hybrid_df['Co-Occurrence Score']\n",
    "hybrid_df = hybrid_df.sort_values(by='Hybrid Score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# === Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(hybrid_df['ProductID'], hybrid_df['Hybrid Score'], color='purple')\n",
    "plt.xlabel(\"Hybrid Score\")\n",
    "plt.title(f\"Top Recommended Products for User {sample_user_id}\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Display Result ===\n",
    "print(f\"âœ… Top Hybrid Recommendations for User {sample_user_id}:\\n\")\n",
    "print(hybrid_df)\n",
    "# === Step 1: Load preprocessed clickstream data ===\n",
    "df = pd.read_csv(\"final_preprocessed_data.csv\")\n",
    "\n",
    "# === Step 2: Focus on 'product_view' events only ===\n",
    "product_views = df[df['EventType'] == 'product_view']\n",
    "\n",
    "# === Step 3: Count how many times each user viewed each product ===\n",
    "view_counts = product_views.groupby(['UserID', 'ProductID']).size().reset_index(name='ViewCount')\n",
    "\n",
    "# === Step 4: Sort by highest view count (intent) ===\n",
    "sorted_views = view_counts.sort_values(by='ViewCount', ascending=False)\n",
    "\n",
    "# === Step 5: Get top product (most-viewed) for 10 unique users ===\n",
    "top_10_intent_unique = sorted_views.drop_duplicates(subset='UserID', keep='first').head(10)\n",
    "\n",
    "# === Step 6: Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_intent_unique['UserID'].astype(str), top_10_intent_unique['ViewCount'], color='teal')\n",
    "plt.xlabel(\"View Count\")\n",
    "plt.ylabel(\"UserID\")\n",
    "plt.title(\"ðŸŽ¯ Top 10 High-Intent Users and Their Most Viewed Products\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Step 7: Print final recommendation table ===\n",
    "print(\"âœ… Recommended Products for High-Intent Users:\")\n",
    "print(top_10_intent_unique)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37d3f1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 3: Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_product_matrix = purchase_data.groupby(['UserID', 'ProductID']).size().unstack(fill_value=0)\n",
    "\n",
    "# === Step 2: Collaborative Filtering (TruncatedSVD) ===\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(user_product_matrix)\n",
    "user_similarity = cosine_similarity(user_factors)\n",
    "\n",
    "# === Step 3: Co-Occurrence Matrix ===\n",
    "user_purchases = purchase_data.groupby('UserID')['ProductID'].apply(set)\n",
    "co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for products in user_purchases:\n",
    "    for prod_a, prod_b in combinations(products, 2):\n",
    "        co_occurrence[prod_a][prod_b] += 1\n",
    "        co_occurrence[prod_b][prod_a] += 1\n",
    "\n",
    "# === Step 4: Target User Selection ===\n",
    "user_ids = user_product_matrix.index\n",
    "sample_user_index = 0\n",
    "sample_user_id = user_ids[sample_user_index]\n",
    "user_products = user_purchases[sample_user_id]\n",
    "\n",
    "# --- A: Collaborative Filtering Recommendations ---\n",
    "similar_user_indices = user_similarity[sample_user_index].argsort()[::-1][1:6]\n",
    "similar_user_ids = user_ids[similar_user_indices]\n",
    "\n",
    "collab_scores = user_product_matrix.loc[similar_user_ids].sum()\n",
    "collab_scores = collab_scores.drop(labels=user_products, errors='ignore')\n",
    "collab_scores = collab_scores[collab_scores > 0].sort_values(ascending=False)\n",
    "collab_df = collab_scores.reset_index()\n",
    "collab_df.columns = ['ProductID', 'Collaborative Score']\n",
    "\n",
    "# --- B: Co-Occurrence Recommendations ---\n",
    "cooccur_scores = defaultdict(int)\n",
    "for product in user_products:\n",
    "    for related, score in co_occurrence[product].items():\n",
    "        if related not in user_products:\n",
    "            cooccur_scores[related] += score\n",
    "\n",
    "cooccur_df = pd.DataFrame.from_dict(cooccur_scores, orient='index', columns=['Co-Occurrence Score'])\n",
    "cooccur_df = cooccur_df.sort_values(by='Co-Occurrence Score', ascending=False).reset_index()\n",
    "cooccur_df.columns = ['ProductID', 'Co-Occurrence Score']\n",
    "\n",
    "# --- C: Hybrid Ranking ---\n",
    "hybrid_df = pd.merge(collab_df, cooccur_df, on='ProductID', how='outer').fillna(0)\n",
    "hybrid_df['Hybrid Score'] = hybrid_df['Collaborative Score'] + hybrid_df['Co-Occurrence Score']\n",
    "hybrid_df = hybrid_df.sort_values(by='Hybrid Score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# === Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(hybrid_df['ProductID'], hybrid_df['Hybrid Score'], color='purple')\n",
    "plt.xlabel(\"Hybrid Score\")\n",
    "plt.title(f\"Top Recommended Products for User {sample_user_id}\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Display Result ===\n",
    "print(f\"âœ… Top Hybrid Recommendations for User {sample_user_id}:\\n\")\n",
    "print(hybrid_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a854e5",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 4: High-Intent Product Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"final_preprocessed_data.csv\")\n",
    "\n",
    "# === Step 2: Focus on 'product_view' events only ===\n",
    "product_views = df[df['EventType'] == 'product_view']\n",
    "\n",
    "# === Step 3: Count how many times each user viewed each product ===\n",
    "view_counts = product_views.groupby(['UserID', 'ProductID']).size().reset_index(name='ViewCount')\n",
    "\n",
    "# === Step 4: Sort by highest view count (intent) ===\n",
    "sorted_views = view_counts.sort_values(by='ViewCount', ascending=False)\n",
    "\n",
    "# === Step 5: Get top product (most-viewed) for 10 unique users ===\n",
    "top_10_intent_unique = sorted_views.drop_duplicates(subset='UserID', keep='first').head(10)\n",
    "\n",
    "# === Step 6: Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_intent_unique['UserID'].astype(str), top_10_intent_unique['ViewCount'], color='teal')\n",
    "plt.xlabel(\"View Count\")\n",
    "plt.ylabel(\"UserID\")\n",
    "plt.title(\"ðŸŽ¯ Top 10 High-Intent Users and Their Most Viewed Products\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Step 7: Print final recommendation table ===\n",
    "print(\"âœ… Recommended Products for High-Intent Users:\")\n",
    "print(top_10_intent_unique)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9d8d6",
   "metadata": {},
   "source": [
    "## ðŸš¨ Step 5: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfm_df = pd.read_csv(\"final_with_rfm.csv\")  # For RFM anomaly detection\n",
    "df = pd.read_csv(\"final_preprocessed_data.csv\")  # For login/click analysis\n",
    "\n",
    "# ==================== PART 1: RFM ANOMALY DETECTION ====================\n",
    "\n",
    "# --- Scale RFM features ---\n",
    "rfm_features = rfm_df[['recency', 'frequency', 'monetary']]\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_features)\n",
    "\n",
    "# --- Isolation Forest ---\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "rfm_df['Anomaly'] = iso_forest.fit_predict(rfm_scaled)\n",
    "\n",
    "# --- Extract anomalies ---\n",
    "rfm_anomalies = rfm_df[rfm_df['Anomaly'] == -1].copy()\n",
    "rfm_normals = rfm_df[rfm_df['Anomaly'] == 1].copy()\n",
    "\n",
    "# --- Visualize cluster anomalies ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=rfm_normals, x='recency', y='monetary', label='Normal', alpha=0.6)\n",
    "sns.scatterplot(data=rfm_anomalies, x='recency', y='monetary', color='red', label='Anomaly', s=100, edgecolor='black')\n",
    "plt.title(\"ðŸ§  RFM Anomaly Clusters (Isolation Forest)\")\n",
    "plt.xlabel(\"Recency\")\n",
    "plt.ylabel(\"Monetary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== PART 2: CLICK / LOGIN SPIKE DETECTION ====================\n",
    "\n",
    "# --- Filter suspicious events ---\n",
    "suspicious_df = df[df['EventType'].isin(['click', 'login'])]\n",
    "\n",
    "# --- Count events per user ---\n",
    "user_event_counts = suspicious_df.groupby('UserID').size().reset_index(name='EventCount')\n",
    "\n",
    "# --- Calculate threshold and flag top activity users ---\n",
    "threshold = user_event_counts['EventCount'].quantile(0.95)\n",
    "anomalous_users = user_event_counts[user_event_counts['EventCount'] > threshold]\n",
    "\n",
    "# --- Show top suspicious users ---\n",
    "print(f\"ðŸ“ˆ 95th percentile threshold: {threshold:.2f}\")\n",
    "print(\"ðŸš¨ High Click/Login Activity Users:\")\n",
    "print(anomalous_users.sort_values(by='EventCount', ascending=False).head(10))\n",
    "\n",
    "# --- Optional: Timeline view for top suspicious user(s) ---\n",
    "top_ids = anomalous_users['UserID'].head(3).tolist()\n",
    "timeline_df = df[df['UserID'].isin(top_ids)][['UserID', 'EventType', 'Timestamp']]\n",
    "timeline_df['Timestamp'] = pd.to_datetime(timeline_df['Timestamp'])\n",
    "timeline_df = timeline_df.sort_values(by=['UserID', 'Timestamp'])\n",
    "\n",
    "print(\"\\nâ±ï¸ Timeline Events for Top Suspicious Users:\")\n",
    "print(timeline_df.head(20))\n",
    "\n",
    "# Visualize Event Anomalies (Pie)\n",
    "# Extract anomalies\n",
    "event_anomalous_users = user_event_counts[user_event_counts['EventCount'] > threshold]\n",
    "event_counts = pd.Series({\n",
    "    'Normal Users': df['UserID'].nunique() - event_anomalous_users['UserID'].nunique(),\n",
    "    'Event Spike Anomalies': event_anomalous_users['UserID'].nunique()\n",
    "})\n",
    "\n",
    "colors = ['#b3e6ff', '#ff9999']\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(event_counts, labels=event_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "plt.title(\"ðŸ›‘ Click/Login Spike Anomaly Distribution\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ“¤ EXPORT RESULTS\n",
    "# =============================================================================\n",
    "rfm_anomalies.to_csv(\"rfm_anomalies_users.csv\", index=False)\n",
    "event_anomalous_users.to_csv(\"event_spike_anomalies_users.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Anomaly Detection & Fraud Reports Exported:\")\n",
    "print(\"- rfm_anomalies_users.csv\")\n",
    "print(\"- event_spike_anomalies_users.csv\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85872ee4",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 6: Final Dashboard Summary & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c03947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfm_anomalies = pd.read_csv(\"rfm_anomalies_users.csv\")\n",
    "event_anomalies = pd.read_csv(\"event_spike_anomalies_users.csv\")\n",
    "churn_df = pd.read_csv(\"customer_marketing_actions.csv\")\n",
    "\n",
    "# Load cluster assignments\n",
    "cluster_0 = pd.read_csv(\"cluster_0_customers.csv\")\n",
    "cluster_1 = pd.read_csv(\"cluster_1_customers.csv\")\n",
    "cluster_2 = pd.read_csv(\"cluster_2_customers.csv\")\n",
    "\n",
    "# Assign cluster labels\n",
    "cluster_0[\"Cluster\"] = 0\n",
    "cluster_1[\"Cluster\"] = 1\n",
    "cluster_2[\"Cluster\"] = 2\n",
    "cluster_all = pd.concat([cluster_0, cluster_1, cluster_2], ignore_index=True)\n",
    "\n",
    "# === STEP 1: Build User-Level Summary Table ===\n",
    "summary = cluster_all[[\"UserID\", \"Cluster\"]].drop_duplicates()\n",
    "\n",
    "# Churn status: infer from marketing actions\n",
    "churn_df[\"Churn\"] = churn_df[\"Marketing_Action\"].str.contains(\"Win-Back|At-Risk\", case=False, na=False).astype(int)\n",
    "summary = summary.merge(churn_df[[\"UserID\", \"Churn\"]], on=\"UserID\", how=\"left\")\n",
    "\n",
    "# Add anomaly flags\n",
    "summary[\"RFM_Anomaly\"] = summary[\"UserID\"].isin(rfm_anomalies[\"UserID\"])\n",
    "summary[\"Event_Anomaly\"] = summary[\"UserID\"].isin(event_anomalies[\"UserID\"])\n",
    "\n",
    "# === STEP 2: Visualization Dashboard ===\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"ðŸ“Š Final Project Snapshot â€“ Segmentation, Churn, Anomaly\", fontsize=16)\n",
    "\n",
    "# 1. Customer Segmentation (Cluster Distribution)\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(x=\"Cluster\", data=summary, palette=\"Set2\")\n",
    "plt.title(\"ðŸ“¦ Customer Segmentation (K-Means Clusters)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"User Count\")\n",
    "\n",
    "# 2. Churn Prediction (Pie Chart)\n",
    "plt.subplot(2, 2, 2)\n",
    "churn_counts = summary[\"Churn\"].value_counts().rename({0: \"Active\", 1: \"Churned\"})\n",
    "plt.pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', startangle=90, colors=[\"#66b3ff\", \"#ff9999\"])\n",
    "plt.title(\"ðŸ” Churn Prediction Status\")\n",
    "\n",
    "# 3. RFM Anomaly (Pie Chart)\n",
    "plt.subplot(2, 2, 3)\n",
    "rfm_counts = summary[\"RFM_Anomaly\"].value_counts().rename({True: \"Anomaly\", False: \"Normal\"})\n",
    "plt.pie(rfm_counts, labels=rfm_counts.index, autopct='%1.1f%%', startangle=90, colors=[\"#ff6666\", \"#99ffcc\"])\n",
    "plt.title(\"ðŸ§  RFM-Based Anomaly Rate\")\n",
    "\n",
    "# 4. Event-Based Anomaly (Pie Chart)\n",
    "plt.subplot(2, 2, 4)\n",
    "event_counts = summary[\"Event_Anomaly\"].value_counts().rename({True: \"Spike Detected\", False: \"Normal\"})\n",
    "plt.pie(event_counts, labels=event_counts.index, autopct='%1.1f%%', startangle=90, colors=[\"#ffcc99\", \"#cce5ff\"])\n",
    "plt.title(\"ðŸ–±ï¸ Click/Login Spike Detection\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# === STEP 3: Print Textual Summary ===\n",
    "print(\"\\nðŸ“‹ PROJECT SUMMARY\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"ðŸ‘¥ Total Users: {summary['UserID'].nunique()}\")\n",
    "print(f\"ðŸ“¦ Segmentation Clusters: {summary['Cluster'].nunique()}\")\n",
    "print(f\"ðŸ” Users Predicted to Churn: {summary['Churn'].sum()}\")\n",
    "print(f\"ðŸ§  RFM-Based Anomalies: {summary['RFM_Anomaly'].sum()}\")\n",
    "print(f\"ðŸ–±ï¸ Event-Based Spike Anomalies: {summary['Event_Anomaly'].sum()}\")\n",
    "print(\"ðŸŽ¯ Recommendation System: Implemented (Collaborative + Co-Occurrence)\")\n",
    "print(\"âœ… Final Pipeline Complete!\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Sample data for churn\n",
    "labels = ['Churned', 'Active']\n",
    "values = [25.5, 74.5]  # You can replace with your real churn percentages\n",
    "\n",
    "# Custom poster-themed colors\n",
    "colors = ['#FF6F61', '#6A67CE']  # orange-red and purple\n",
    "\n",
    "fig = px.pie(\n",
    "    names=labels,\n",
    "    values=values,\n",
    "    title=\"ðŸ“‰ Churn Distribution\",\n",
    "    color_discrete_sequence=colors,\n",
    "    hole=0.3  # for donut effect if preferred\n",
    ")\n",
    "\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.update_layout(\n",
    "    font=dict(size=16),\n",
    "    showlegend=True,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "pip install -U kaleido\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Pie Chart: Churn Distribution\n",
    "fig_pie = go.Figure(data=[go.Pie(\n",
    "    labels=['Active', 'Churned'],\n",
    "    values=[76.5, 23.5],\n",
    "    hole=0.5,\n",
    "    marker=dict(colors=['#f25f5c', '#3a0ca3']),  # Poster palette\n",
    "    textinfo='label+percent',\n",
    ")])\n",
    "fig_pie.update_layout(\n",
    "    title='Churn Distribution',\n",
    "    showlegend=True,\n",
    "    paper_bgcolor='rgba(0,0,0,0)',  # Transparent\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    font=dict(color='#1d3557')\n",
    ")\n",
    "\n",
    "# Funnel Chart: User Journey\n",
    "stages = ['login', 'page_view', 'product_view', 'click', 'add_to_cart', 'purchase']\n",
    "values = [1000, 521, 494, 512, 468, 450]\n",
    "\n",
    "fig_funnel = go.Figure(go.Funnel(\n",
    "    y=stages,\n",
    "    x=values,\n",
    "    marker=dict(color=['#ef476f', '#ffd166', '#06d6a0', '#118ab2', '#073b4c', '#3a0ca3'])  # Poster colors\n",
    "))\n",
    "fig_funnel.update_layout(\n",
    "    title='User Funnel Analysis',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    font=dict(color='#1d3557')\n",
    ")\n",
    "\n",
    "# Show plots (Use .write_image() if exporting to file)\n",
    "fig_pie.show()\n",
    "fig_funnel.show()\n",
    "\n",
    "fig_pie.write_image(\"churn_pie_chart.png\")\n",
    "fig_funnel.write_image(\"user_funnel_chart.png\")\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Marketing Action': [\n",
    "        'Loyal Customers: Personalized product recommendations.',\n",
    "        'VIP Customers: Exclusive discounts & early sales access.',\n",
    "        'At-Risk Customers: Win-back campaigns with incentives.'\n",
    "    ],\n",
    "    'Number of Customers': [29500, 22500, 6800]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig_bar = px.bar(\n",
    "    df,\n",
    "    x='Number of Customers',\n",
    "    y='Marketing Action',\n",
    "    orientation='h',\n",
    "    color='Marketing Action',\n",
    "    color_discrete_sequence=['#FF6B6B', '#FFD93D', '#6BCB77'],  # Custom colors\n",
    "    title='Customer Count by Marketing Action'\n",
    ")\n",
    "\n",
    "# Make text and lines black for poster integration\n",
    "fig_bar.update_layout(\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    font=dict(color='black', size=14),\n",
    "    showlegend=False,\n",
    "    xaxis=dict(\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        gridcolor='black', zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        gridcolor='black', zeroline=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to file (transparent background, black lines)\n",
    "fig_bar.write_image(\"bar_marketing_action_black.png\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the nodes (steps)\n",
    "labels = [\n",
    "    \"Data Collection\", \"Preprocessing\", \"RFM Analysis\", \"PCA\", \"K-Means Clustering\",\n",
    "    \"Customer Segments\", \"Churn Prediction\", \"Funnel Analysis\", \"Recommendation Engine\",\n",
    "    \"Anomaly Detection\", \"Streamlit Dashboard\"\n",
    "]\n",
    "\n",
    "# Define directional flow: source index â†’ target index\n",
    "source = [0, 1, 2, 3, 4, 5, 5, 5, 5, 6, 7, 8, 9]\n",
    "target = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "\n",
    "# Uniform flow value for simplicity\n",
    "value = [1]*len(source)\n",
    "\n",
    "# Build the Sankey Diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels,\n",
    "        color=\"rgba(173, 216, 230, 0.8)\"\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=\"rgba(100, 149, 237, 0.4)\"\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Project Methodology Workflow (Sankey Diagram)\",\n",
    "    font_size=13,\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"methodology_sankey_diagram.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
